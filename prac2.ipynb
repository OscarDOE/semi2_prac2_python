{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "Click <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = pd.read_csv('/content/Datos.csv')\n",
    "\n",
    "# Verificar los datos cargados y observar las primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Revisar si hay valores nulos\n",
    "print(\"Valores nulos por columna antes de la limpieza:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Reemplazar valores nulos en la columna 'Rating' con 0\n",
    "df['Rating'] = df['Rating'].fillna(0)\n",
    "\n",
    "# Verificar que se hayan reemplazado los valores nulos\n",
    "print(\"Valores nulos por columna después de la limpieza:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los cursos por 'Course Title' y calcular el promedio de 'Rating'\n",
    "promedios = df.groupby('Course Title')['Rating'].mean()\n",
    "\n",
    "# Mostrar los promedios de calificaciones\n",
    "print(\"Promedio de calificaciones por curso:\")\n",
    "print(promedios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curso con mayor rating\n",
    "curso_mayor_rating = df[df['Rating'] == df['Rating'].max()]\n",
    "\n",
    "# Curso con menor rating\n",
    "curso_menor_rating = df[df['Rating'] == df['Rating'].replace(0, float('nan')).min()]\n",
    "\n",
    "print(\"Curso con mayor calificación:\")\n",
    "print(curso_mayor_rating[['Course Title', 'Rating']])\n",
    "\n",
    "print(\"Curso con menor calificación:\")\n",
    "print(curso_menor_rating[['Course Title', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los cursos con horario flexible\n",
    "cursos_flexibles = df[df['Schedule'] == 'Flexible schedule']\n",
    "\n",
    "# Calcular el porcentaje de cursos flexibles en relación al total\n",
    "porcentaje_flexibles = (len(cursos_flexibles) / len(df)) * 100\n",
    "\n",
    "# Mostrar el porcentaje\n",
    "print(f\"Porcentaje de cursos con horario flexible: {porcentaje_flexibles:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gráfica de barras: Número de cursos por nivel de dificultad\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Level'].value_counts().plot(kind='bar')\n",
    "plt.title('Número de cursos por nivel de dificultad')\n",
    "plt.xlabel('Nivel de dificultad')\n",
    "plt.ylabel('Número de cursos')\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de barras horizontal: Número de cursos por categoría\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Keyword'].value_counts().plot(kind='barh')\n",
    "plt.title('Número de cursos por categoría')\n",
    "plt.xlabel('Número de cursos')\n",
    "plt.ylabel('Categoría')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de dispersión: Relación entre duración y número de revisiones\n",
    "df['Review'] = df['Review'].str.replace(' reviews', '').str.replace(',', '').astype(float)  # Limpiar datos\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['Duration'].str.extract('(\\d+)').astype(float), df['Review'])\n",
    "plt.title('Relación entre duración del curso y número de revisiones')\n",
    "plt.xlabel('Duración del curso (horas)')\n",
    "plt.ylabel('Número de revisiones')\n",
    "plt.show()\n",
    "\n",
    "# Histograma: Distribución de las duraciones de los cursos\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Duration'].str.extract('(\\d+)').astype(float).plot(kind='hist', bins=10)\n",
    "plt.title('Distribución de las duraciones de los cursos')\n",
    "plt.xlabel('Duración (horas)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de cajas: Distribución de las calificaciones por nivel de dificultad\n",
    "plt.figure(figsize=(10, 5))\n",
    "df.boxplot(column='Rating', by='Level')\n",
    "plt.title('Distribución de calificaciones por nivel de dificultad')\n",
    "plt.suptitle('')  # Eliminar el título automático de boxplot\n",
    "plt.xlabel('Nivel de dificultad')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Descargar los recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar el modelo de spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.max_length = len(data)\n",
    "\n",
    "# Cargar el archivo de texto\n",
    "with open(\"Coursera Comments.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# 1. Tokenización\n",
    "tokens = word_tokenize(data)\n",
    "print(\"Tokenización:\", tokens[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 2. Lematización (remover signos de puntuación)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha()]\n",
    "print(\"Lematización:\", lemmatized_words[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 3. Eliminar palabras vacías (stopwords)\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "filtered_words = [word for word in lemmatized_words if word not in spanish_stopwords]\n",
    "print(\"Palabras después de eliminar palabras vacías:\", filtered_words[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 4. Frecuencia de palabras\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Palabras más comunes:\", word_freq.most_common(10))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 5. Análisis de sentimientos\n",
    "blob = TextBlob(data)\n",
    "sentimiento = blob.sentiment\n",
    "print(\"Análisis de Sentimientos:\")\n",
    "print(\"Polaridad:\", sentimiento.polarity)\n",
    "print(\"Subjetividad:\", sentimiento.subjectivity)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 6. Reconocimiento de entidades nombradas con spaCy\n",
    "doc = nlp(data)\n",
    "entidades_nombradas = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entidades nombradas:\", entidades_nombradas[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 7. Extraer entidades por tipo\n",
    "entidades_personas = [ent.text for ent in doc.ents if ent.label_ == 'PER']\n",
    "entidades_organizaciones = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "entidades_fechas = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "\n",
    "print(\"Entidades personas:\", entidades_personas[:5])\n",
    "print(\"Entidades organizaciones:\", entidades_organizaciones[:5])\n",
    "print(\"Entidades fechas:\", entidades_fechas[:5])\n",
    "print(\"------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 📊 Explicación del Proceso de Análisis de Datos (Archivo .csv)\n",
    "\n",
    "### 1. **Lectura del archivo CSV**\n",
    "Primero cargamos el archivo CSV, que es un formato muy común para trabajar con datos estructurados. Es como una tabla en la que cada fila tiene diferentes valores separados por comas.\n",
    "\n",
    "### 2. **Exploración de los datos**\n",
    "Revisamos los datos para entender qué columnas tenemos y qué tipo de información está contenida. Es como dar una primera ojeada antes de empezar a trabajar en serio.\n",
    "\n",
    "### 3. **Limpieza de datos**\n",
    "Aquí eliminamos valores faltantes o duplicados y corregimos cualquier error que pudiera haber. Esto es crucial porque los datos \"sucios\" pueden arruinar el análisis.\n",
    "\n",
    "### 4. **Análisis descriptivo**\n",
    "Luego calculamos algunas estadísticas básicas, como promedios y frecuencias. Esto nos da una visión inicial de cómo están distribuidos los datos.\n",
    "\n",
    "### 5. **Visualización de datos**\n",
    "Creamos gráficos para ver mejor la información y descubrir patrones que no son tan fáciles de ver en tablas.\n",
    "\n",
    "### 6. **Análisis avanzado**\n",
    "Dependiendo de lo que se necesite, se puede hacer un análisis más profundo, como agrupar los datos o incluso hacer predicciones sobre el futuro usando modelos de machine learning.\n",
    "\n",
    "### 7. **Conclusión de los resultados**\n",
    "Finalmente, interpretamos todo lo que descubrimos para sacar conclusiones y hacer recomendaciones basadas en los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Explicación del Proceso de Análisis de Texto (Archivo .txt)\n",
    "\n",
    "### 1. **Lectura del archivo**\n",
    "Primero leímos el archivo completo en Python. Este paso es súper sencillo, pero muy importante, porque necesitamos tener todo el texto en una variable para poder trabajar con él.\n",
    "\n",
    "### 2. **Tokenización**\n",
    "Aquí separamos el texto en palabras individuales o \"tokens\". Esto nos permite analizar cada palabra por separado. Imagínate dividir una frase como \"Hola, mundo\" en dos palabras: \"Hola\" y \"mundo\".\n",
    "\n",
    "### 3. **Lematización y Stemming**\n",
    "Luego pasamos a la lematización, que es como reducir las palabras a su forma base. Por ejemplo, \"corriendo\" se convierte en \"correr\". Así podemos agrupar palabras similares y hacer un análisis más eficiente.\n",
    "\n",
    "### 4. **Eliminación de palabras vacías**\n",
    "En este paso eliminamos palabras que no aportan mucho al significado, como \"de\", \"el\", \"la\", etc. Esto ayuda a concentrarnos en las palabras que realmente importan en el texto.\n",
    "\n",
    "### 5. **Frecuencia de palabras**\n",
    "Después, contamos cuántas veces aparece cada palabra en el texto. Esto nos ayuda a identificar las palabras más comunes y a entender los temas más discutidos.\n",
    "\n",
    "### 6. **Análisis de sentimientos**\n",
    "Con el análisis de sentimientos, calculamos la polaridad y la subjetividad del texto. En otras palabras, ¿es un texto positivo o negativo? ¿Es objetivo o subjetivo? Esto nos da una idea de cómo se sienten los autores sobre los temas tratados.\n",
    "\n",
    "### 7. **Reconocimiento de entidades nombradas**\n",
    "Finalmente, identificamos entidades importantes, como nombres de personas, lugares, fechas, etc. Esto nos da una visión más estructurada del texto y permite identificar elementos clave.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔍 Conclusión General\n",
    "\n",
    "Después de hacer todo el análisis de texto, podemos decir que Python es una herramienta súper útil para trabajar con grandes cantidades de datos, sobre todo en temas de análisis de texto. Usando varias técnicas como **tokenización**, **lematización**, la eliminación de **palabras vacías** y el análisis de **sentimientos** y **entidades**, hemos logrado obtener información valiosa del archivo.\n",
    "\n",
    "Python facilita mucho el trabajo cuando se trata de manipular textos, ya que tiene bibliotecas poderosas como **spaCy**, **NLTK**, y **TextBlob** que hacen casi todo el trabajo pesado por ti. Además, este análisis no solo te permite entender de qué trata un texto, sino que también te ayuda a obtener el \"sentir\" general de los autores de esos textos.\n",
    "\n",
    "Al final del día, el poder de Python no se limita al análisis de texto, sino que también es ideal para manejar datos en general. Y lo mejor es que sigue mejorando, gracias a la comunidad y las herramientas que siempre se están actualizando.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "💡 **En resumen**, Python no solo es poderoso, sino que también es súper flexible. Nos permite hacer de todo, desde el análisis de texto hasta trabajar con datos estructurados como tablas. Y lo mejor es que hay una biblioteca para casi cualquier cosa que necesites hacer. 🎉\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c98ef0526993bcb5196b2568a6e46a301af2b9ce6e004586deeecfee39fb7e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
