{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "Click <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame\n",
    "df = pd.read_csv('/content/Datos.csv')\n",
    "\n",
    "# Verificar los datos cargados y observar las primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Revisar si hay valores nulos\n",
    "print(\"Valores nulos por columna antes de la limpieza:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Reemplazar valores nulos en la columna 'Rating' con 0\n",
    "df['Rating'] = df['Rating'].fillna(0)\n",
    "\n",
    "# Verificar que se hayan reemplazado los valores nulos\n",
    "print(\"Valores nulos por columna despu칠s de la limpieza:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los cursos por 'Course Title' y calcular el promedio de 'Rating'\n",
    "promedios = df.groupby('Course Title')['Rating'].mean()\n",
    "\n",
    "# Mostrar los promedios de calificaciones\n",
    "print(\"Promedio de calificaciones por curso:\")\n",
    "print(promedios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curso con mayor rating\n",
    "curso_mayor_rating = df[df['Rating'] == df['Rating'].max()]\n",
    "\n",
    "# Curso con menor rating\n",
    "curso_menor_rating = df[df['Rating'] == df['Rating'].replace(0, float('nan')).min()]\n",
    "\n",
    "print(\"Curso con mayor calificaci칩n:\")\n",
    "print(curso_mayor_rating[['Course Title', 'Rating']])\n",
    "\n",
    "print(\"Curso con menor calificaci칩n:\")\n",
    "print(curso_menor_rating[['Course Title', 'Rating']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los cursos con horario flexible\n",
    "cursos_flexibles = df[df['Schedule'] == 'Flexible schedule']\n",
    "\n",
    "# Calcular el porcentaje de cursos flexibles en relaci칩n al total\n",
    "porcentaje_flexibles = (len(cursos_flexibles) / len(df)) * 100\n",
    "\n",
    "# Mostrar el porcentaje\n",
    "print(f\"Porcentaje de cursos con horario flexible: {porcentaje_flexibles:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gr치fica de barras: N칰mero de cursos por nivel de dificultad\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Level'].value_counts().plot(kind='bar')\n",
    "plt.title('N칰mero de cursos por nivel de dificultad')\n",
    "plt.xlabel('Nivel de dificultad')\n",
    "plt.ylabel('N칰mero de cursos')\n",
    "plt.show()\n",
    "\n",
    "# Gr치fica de barras horizontal: N칰mero de cursos por categor칤a\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Keyword'].value_counts().plot(kind='barh')\n",
    "plt.title('N칰mero de cursos por categor칤a')\n",
    "plt.xlabel('N칰mero de cursos')\n",
    "plt.ylabel('Categor칤a')\n",
    "plt.show()\n",
    "\n",
    "# Gr치fico de dispersi칩n: Relaci칩n entre duraci칩n y n칰mero de revisiones\n",
    "df['Review'] = df['Review'].str.replace(' reviews', '').str.replace(',', '').astype(float)  # Limpiar datos\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['Duration'].str.extract('(\\d+)').astype(float), df['Review'])\n",
    "plt.title('Relaci칩n entre duraci칩n del curso y n칰mero de revisiones')\n",
    "plt.xlabel('Duraci칩n del curso (horas)')\n",
    "plt.ylabel('N칰mero de revisiones')\n",
    "plt.show()\n",
    "\n",
    "# Histograma: Distribuci칩n de las duraciones de los cursos\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Duration'].str.extract('(\\d+)').astype(float).plot(kind='hist', bins=10)\n",
    "plt.title('Distribuci칩n de las duraciones de los cursos')\n",
    "plt.xlabel('Duraci칩n (horas)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# Gr치fico de cajas: Distribuci칩n de las calificaciones por nivel de dificultad\n",
    "plt.figure(figsize=(10, 5))\n",
    "df.boxplot(column='Rating', by='Level')\n",
    "plt.title('Distribuci칩n de calificaciones por nivel de dificultad')\n",
    "plt.suptitle('')  # Eliminar el t칤tulo autom치tico de boxplot\n",
    "plt.xlabel('Nivel de dificultad')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Descargar los recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar el modelo de spaCy en espa침ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.max_length = len(data)\n",
    "\n",
    "# Cargar el archivo de texto\n",
    "with open(\"Coursera Comments.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# 1. Tokenizaci칩n\n",
    "tokens = word_tokenize(data)\n",
    "print(\"Tokenizaci칩n:\", tokens[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 2. Lematizaci칩n (remover signos de puntuaci칩n)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha()]\n",
    "print(\"Lematizaci칩n:\", lemmatized_words[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 3. Eliminar palabras vac칤as (stopwords)\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "filtered_words = [word for word in lemmatized_words if word not in spanish_stopwords]\n",
    "print(\"Palabras despu칠s de eliminar palabras vac칤as:\", filtered_words[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 4. Frecuencia de palabras\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Palabras m치s comunes:\", word_freq.most_common(10))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 5. An치lisis de sentimientos\n",
    "blob = TextBlob(data)\n",
    "sentimiento = blob.sentiment\n",
    "print(\"An치lisis de Sentimientos:\")\n",
    "print(\"Polaridad:\", sentimiento.polarity)\n",
    "print(\"Subjetividad:\", sentimiento.subjectivity)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 6. Reconocimiento de entidades nombradas con spaCy\n",
    "doc = nlp(data)\n",
    "entidades_nombradas = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entidades nombradas:\", entidades_nombradas[:10])\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 7. Extraer entidades por tipo\n",
    "entidades_personas = [ent.text for ent in doc.ents if ent.label_ == 'PER']\n",
    "entidades_organizaciones = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
    "entidades_fechas = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "\n",
    "print(\"Entidades personas:\", entidades_personas[:5])\n",
    "print(\"Entidades organizaciones:\", entidades_organizaciones[:5])\n",
    "print(\"Entidades fechas:\", entidades_fechas[:5])\n",
    "print(\"------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 游늵 Explicaci칩n del Proceso de An치lisis de Datos (Archivo .csv)\n",
    "\n",
    "### 1. **Lectura del archivo CSV**\n",
    "Primero cargamos el archivo CSV, que es un formato muy com칰n para trabajar con datos estructurados. Es como una tabla en la que cada fila tiene diferentes valores separados por comas.\n",
    "\n",
    "### 2. **Exploraci칩n de los datos**\n",
    "Revisamos los datos para entender qu칠 columnas tenemos y qu칠 tipo de informaci칩n est치 contenida. Es como dar una primera ojeada antes de empezar a trabajar en serio.\n",
    "\n",
    "### 3. **Limpieza de datos**\n",
    "Aqu칤 eliminamos valores faltantes o duplicados y corregimos cualquier error que pudiera haber. Esto es crucial porque los datos \"sucios\" pueden arruinar el an치lisis.\n",
    "\n",
    "### 4. **An치lisis descriptivo**\n",
    "Luego calculamos algunas estad칤sticas b치sicas, como promedios y frecuencias. Esto nos da una visi칩n inicial de c칩mo est치n distribuidos los datos.\n",
    "\n",
    "### 5. **Visualizaci칩n de datos**\n",
    "Creamos gr치ficos para ver mejor la informaci칩n y descubrir patrones que no son tan f치ciles de ver en tablas.\n",
    "\n",
    "### 6. **An치lisis avanzado**\n",
    "Dependiendo de lo que se necesite, se puede hacer un an치lisis m치s profundo, como agrupar los datos o incluso hacer predicciones sobre el futuro usando modelos de machine learning.\n",
    "\n",
    "### 7. **Conclusi칩n de los resultados**\n",
    "Finalmente, interpretamos todo lo que descubrimos para sacar conclusiones y hacer recomendaciones basadas en los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## 游닇 Explicaci칩n del Proceso de An치lisis de Texto (Archivo .txt)\n",
    "\n",
    "### 1. **Lectura del archivo**\n",
    "Primero le칤mos el archivo completo en Python. Este paso es s칰per sencillo, pero muy importante, porque necesitamos tener todo el texto en una variable para poder trabajar con 칠l.\n",
    "\n",
    "### 2. **Tokenizaci칩n**\n",
    "Aqu칤 separamos el texto en palabras individuales o \"tokens\". Esto nos permite analizar cada palabra por separado. Imag칤nate dividir una frase como \"Hola, mundo\" en dos palabras: \"Hola\" y \"mundo\".\n",
    "\n",
    "### 3. **Lematizaci칩n y Stemming**\n",
    "Luego pasamos a la lematizaci칩n, que es como reducir las palabras a su forma base. Por ejemplo, \"corriendo\" se convierte en \"correr\". As칤 podemos agrupar palabras similares y hacer un an치lisis m치s eficiente.\n",
    "\n",
    "### 4. **Eliminaci칩n de palabras vac칤as**\n",
    "En este paso eliminamos palabras que no aportan mucho al significado, como \"de\", \"el\", \"la\", etc. Esto ayuda a concentrarnos en las palabras que realmente importan en el texto.\n",
    "\n",
    "### 5. **Frecuencia de palabras**\n",
    "Despu칠s, contamos cu치ntas veces aparece cada palabra en el texto. Esto nos ayuda a identificar las palabras m치s comunes y a entender los temas m치s discutidos.\n",
    "\n",
    "### 6. **An치lisis de sentimientos**\n",
    "Con el an치lisis de sentimientos, calculamos la polaridad y la subjetividad del texto. En otras palabras, 쯘s un texto positivo o negativo? 쮼s objetivo o subjetivo? Esto nos da una idea de c칩mo se sienten los autores sobre los temas tratados.\n",
    "\n",
    "### 7. **Reconocimiento de entidades nombradas**\n",
    "Finalmente, identificamos entidades importantes, como nombres de personas, lugares, fechas, etc. Esto nos da una visi칩n m치s estructurada del texto y permite identificar elementos clave.\n",
    "\n",
    "---\n",
    "\n",
    "# 游댌 Conclusi칩n General\n",
    "\n",
    "Despu칠s de hacer todo el an치lisis de texto, podemos decir que Python es una herramienta s칰per 칰til para trabajar con grandes cantidades de datos, sobre todo en temas de an치lisis de texto. Usando varias t칠cnicas como **tokenizaci칩n**, **lematizaci칩n**, la eliminaci칩n de **palabras vac칤as** y el an치lisis de **sentimientos** y **entidades**, hemos logrado obtener informaci칩n valiosa del archivo.\n",
    "\n",
    "Python facilita mucho el trabajo cuando se trata de manipular textos, ya que tiene bibliotecas poderosas como **spaCy**, **NLTK**, y **TextBlob** que hacen casi todo el trabajo pesado por ti. Adem치s, este an치lisis no solo te permite entender de qu칠 trata un texto, sino que tambi칠n te ayuda a obtener el \"sentir\" general de los autores de esos textos.\n",
    "\n",
    "Al final del d칤a, el poder de Python no se limita al an치lisis de texto, sino que tambi칠n es ideal para manejar datos en general. Y lo mejor es que sigue mejorando, gracias a la comunidad y las herramientas que siempre se est치n actualizando.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "游눠 **En resumen**, Python no solo es poderoso, sino que tambi칠n es s칰per flexible. Nos permite hacer de todo, desde el an치lisis de texto hasta trabajar con datos estructurados como tablas. Y lo mejor es que hay una biblioteca para casi cualquier cosa que necesites hacer. 游꿀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c98ef0526993bcb5196b2568a6e46a301af2b9ce6e004586deeecfee39fb7e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
